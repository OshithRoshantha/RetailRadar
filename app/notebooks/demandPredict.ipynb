{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('sparkApp').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "+--------------+-----------+-------------------+---------+---+------+------+----------------+----------+----+---------+--------+---------------+---------+------------+----------------+------------+---------------+--------------+------------+-------+\n",
      "|Transaction_ID|Customer_ID|               City|  Country|Age|Gender|Income|Customer_Segment|      Date|Year|    Month|    Time|Total_Purchases|   Amount|Total_Amount|Product_Category|Product_Type|Shipping_Method|Payment_Method|Order_Status|Ratings|\n",
      "+--------------+-----------+-------------------+---------+---+------+------+----------------+----------+----+---------+--------+---------------+---------+------------+----------------+------------+---------------+--------------+------------+-------+\n",
      "|       1000043|      91680|         Fort Worth|      USA| 19|  Male|   Low|             New|2023-11-23|2023| November| 8:23:26|             10|285.67474|   2856.7476|     Electronics|  Smartphone|       Same-Day|        PayPal|   Delivered|      4|\n",
      "|       1000593|      50458|            Bendigo|Australia| 46|  Male|  High|             New|2023-08-20|2023|   August|14:41:13|              4| 35.53774|   142.15096|      Home Decor|   Furniture|       Same-Day|        PayPal|     Shipped|      5|\n",
      "|       1000596|      61173|         Manchester|       UK| 70|  Male|Medium|         Regular|2023-03-29|2023|    March| 3:02:21|              6|11.047317|     66.2839|      Home Decor|       Tools|       Standard|    Debit Card|   Delivered|      2|\n",
      "|       1000831|      96401|             Mackay|Australia| 20|  Male|   Low|             New|2003-01-23|2023|    March|20:40:26|              2| 58.49707|   116.99414|        Clothing|       Shirt|       Same-Day|   Credit Card|   Delivered|      1|\n",
      "|       1000920|      39799|          Frankfurt|  Germany| 22|  Male|Medium|         Regular|2023-04-16|2023|    April| 1:42:26|              5|46.615414|   233.07706|      Home Decor| Decorations|       Standard|   Credit Card|   Delivered|      1|\n",
      "|       1000986|      89655|            Toronto|   Canada| 36|  Male|  High|         Premium|2023-08-26|2023|   August| 7:40:46|              8|11.351654|    90.81323|     Electronics|  Television|       Same-Day|   Credit Card|  Processing|      5|\n",
      "|       1001000|      33717|       Philadelphia|      USA| 35|Female|  High|         Regular|2012-06-23|2023| December| 4:56:51|              4|196.59247|    786.3699|      Home Decor|   Furniture|       Same-Day|          Cash|   Delivered|      2|\n",
      "|       1001061|      82482|          Frankfurt|  Germany| 22|  Male|Medium|         Regular|2024-01-29|2024|  January|12:54:42|              6| 42.30735|    253.8441|           Books|  Children's|        Express|   Credit Card|   Delivered|      2|\n",
      "|       1001124|      26532|            Bristol|       UK| 26|  Male|   Low|         Regular|2023-10-19|2023|  January|12:11:46|              9|30.121069|   271.08963|     Electronics|      Fridge|       Same-Day|        PayPal|   Delivered|      4|\n",
      "|       1001126|      87037|          Frankfurt|  Germany| 24|Female|   Low|             New|2023-05-23|2023|      May|13:18:01|              9|41.272427|   371.45184|      Home Decor|     Kitchen|       Standard|    Debit Card|   Delivered|      1|\n",
      "|       1001176|      86614|           Edmonton|   Canada| 60|Female|  High|         Premium|2007-12-23|2023|     July|22:38:16|              9|397.16098|    3574.449|     Electronics|      Tablet|       Standard|        PayPal|     Pending|      5|\n",
      "|       1001602|      79403|             Boston|      USA| 23|  Male|  High|         Regular|2023-07-31|2023|     July|17:25:41|             10| 214.5452|    2145.452|     Electronics|      Tablet|        Express|          Cash|     Shipped|      4|\n",
      "|       1001637|      42502|            Chicago|      USA| 34|Female|   Low|             New|2007-10-23|2023|     July|17:26:27|              6|489.96204|   2939.7722|     Electronics|      Tablet|        Express|   Credit Card|   Delivered|      4|\n",
      "|       1001788|      27727|          Melbourne|Australia| 39|  Male|Medium|         Premium|2010-11-23|2023|      May|19:24:58|              7|131.40099|    919.8069|           Books| Non-Fiction|       Same-Day|        PayPal|     Shipped|      4|\n",
      "|       1002001|      96683|Newcastle upon Tyne|       UK| 20|  Male|  High|             New|2023-09-17|2023|September| 0:57:09|              9|264.93164|   2384.3848|           Books|     Fiction|       Standard|          Cash|     Pending|      1|\n",
      "|       1002717|      39198|         Portsmouth|       UK| 19|  Male|Medium|         Regular|2023-03-16|2023|    March| 6:19:06|              5|431.46738|    2157.337|      Home Decor|    Lighting|       Same-Day|        PayPal|   Delivered|      3|\n",
      "|       1003009|      25829|            Bendigo|Australia| 26|  Male|   Low|         Regular|2024-02-17|2024|  January| 0:53:03|              4|12.069369|   48.277477|     Electronics|      Fridge|       Standard|   Credit Card|   Delivered|      4|\n",
      "|       1003037|      49998|         Portsmouth|       UK| 21|Female|Medium|         Regular|2010-12-23|2023|  October|17:35:06|              9|128.80405|   1159.2363|           Books|  Literature|       Standard|          Cash|  Processing|      2|\n",
      "|       1003129|      69104|           Canberra|Australia| 34|Female|Medium|         Regular|2023-05-29|2023|      May|18:01:53|             10|384.14456|   3841.4456|         Grocery|       Water|       Same-Day|        PayPal|     Shipped|      3|\n",
      "|       1003285|      24195|            Windsor|   Canada| 26|  Male|   Low|         Regular|2023-05-29|2023|  January|20:24:55|              9|125.15536|   1126.3982|     Electronics|      Fridge|       Same-Day|   Credit Card|   Delivered|      3|\n",
      "+--------------+-----------+-------------------+---------+---+------+------+----------------+----------+----+---------+--------+---------------+---------+------------+----------------+------------+---------------+--------------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"../../data/processed/cleanedData.parquet\")\n",
    "unique_values_df = df.select(\"Product_Type\").distinct()\n",
    "print(unique_values_df.count())\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+---------------+\n",
      "|        Product_Type|      Date|Total_Purchases|\n",
      "+--------------------+----------+---------------+\n",
      "|            Bathroom|2001-01-24|             95|\n",
      "|             Bedding|2001-01-24|             59|\n",
      "|         BlueStar AC|2001-01-24|             28|\n",
      "|          Children's|2001-01-24|             64|\n",
      "|           Chocolate|2001-01-24|             48|\n",
      "|              Coffee|2001-01-24|             85|\n",
      "|         Decorations|2001-01-24|            185|\n",
      "|               Dress|2001-01-24|             47|\n",
      "|             Fiction|2001-01-24|            253|\n",
      "|              Fridge|2001-01-24|            131|\n",
      "|           Furniture|2001-01-24|            175|\n",
      "|          Headphones|2001-01-24|             69|\n",
      "|              Jacket|2001-01-24|             72|\n",
      "|               Jeans|2001-01-24|            144|\n",
      "|               Juice|2001-01-24|            176|\n",
      "|             Kitchen|2001-01-24|             50|\n",
      "|              Laptop|2001-01-24|             94|\n",
      "|            Lighting|2001-01-24|             89|\n",
      "|          Literature|2001-01-24|             41|\n",
      "|Mitsubishi 1.5 To...|2001-01-24|             90|\n",
      "|         Non-Fiction|2001-01-24|            261|\n",
      "|               Shirt|2001-01-24|             90|\n",
      "|               Shoes|2001-01-24|            182|\n",
      "|              Shorts|2001-01-24|             36|\n",
      "|          Smartphone|2001-01-24|            344|\n",
      "|              Snacks|2001-01-24|             75|\n",
      "|          Soft Drink|2001-01-24|            142|\n",
      "|             T-shirt|2001-01-24|            161|\n",
      "|              Tablet|2001-01-24|            114|\n",
      "|          Television|2001-01-24|            192|\n",
      "|            Thriller|2001-01-24|            128|\n",
      "|               Tools|2001-01-24|            121|\n",
      "|               Water|2001-01-24|            356|\n",
      "|            Bathroom|2001-02-24|             62|\n",
      "|             Bedding|2001-02-24|             92|\n",
      "|         BlueStar AC|2001-02-24|             34|\n",
      "|          Children's|2001-02-24|            107|\n",
      "|           Chocolate|2001-02-24|             76|\n",
      "|              Coffee|2001-02-24|             68|\n",
      "|         Decorations|2001-02-24|            156|\n",
      "|               Dress|2001-02-24|             76|\n",
      "|             Fiction|2001-02-24|            321|\n",
      "|              Fridge|2001-02-24|            141|\n",
      "|           Furniture|2001-02-24|            113|\n",
      "|          Headphones|2001-02-24|             77|\n",
      "|              Jacket|2001-02-24|             76|\n",
      "|               Jeans|2001-02-24|             58|\n",
      "|               Juice|2001-02-24|            164|\n",
      "|             Kitchen|2001-02-24|             97|\n",
      "|              Laptop|2001-02-24|             74|\n",
      "+--------------------+----------+---------------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.groupBy(\"Product_Type\", \"Date\").agg(F.sum(F.col('Total_Purchases')).alias(\"Total_Purchases\")).orderBy(\"Date\", \"Product_Type\")\n",
    "df.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"sales_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------+---------------+\n",
      "|Product_Category|      Date|Total_Purchases|\n",
      "+----------------+----------+---------------+\n",
      "|         Grocery|2001-01-24|            882|\n",
      "|         Grocery|2001-01-25|              0|\n",
      "|         Grocery|2001-01-26|              0|\n",
      "|         Grocery|2001-01-27|              0|\n",
      "|         Grocery|2001-01-28|              0|\n",
      "|         Grocery|2001-01-29|              0|\n",
      "|         Grocery|2001-01-30|              0|\n",
      "|         Grocery|2001-01-31|              0|\n",
      "|         Grocery|2001-02-01|              0|\n",
      "|         Grocery|2001-02-02|              0|\n",
      "|         Grocery|2001-02-03|              0|\n",
      "|         Grocery|2001-02-04|              0|\n",
      "|         Grocery|2001-02-05|              0|\n",
      "|         Grocery|2001-02-06|              0|\n",
      "|         Grocery|2001-02-07|              0|\n",
      "|         Grocery|2001-02-08|              0|\n",
      "|         Grocery|2001-02-09|              0|\n",
      "|         Grocery|2001-02-10|              0|\n",
      "|         Grocery|2001-02-11|              0|\n",
      "|         Grocery|2001-02-12|              0|\n",
      "+----------------+----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "minMaxDates = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        Product_Category, \n",
    "        MIN(Date) AS min_date, \n",
    "        MAX(Date) AS max_date \n",
    "    FROM sales_data \n",
    "    GROUP BY Product_Category\n",
    "\"\"\")\n",
    "minMaxDates.createOrReplaceTempView(\"minMaxDates\")\n",
    "\n",
    "\n",
    "dateSeries = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        Product_Category, \n",
    "        date_add(min_date, idx) AS Date\n",
    "    FROM (\n",
    "        SELECT \n",
    "            Product_Category, \n",
    "            min_date, \n",
    "            max_date, \n",
    "            posexplode(\n",
    "                split(space(datediff(max_date, min_date)), ' ')\n",
    "            ) AS (idx, _)\n",
    "        FROM minMaxDates\n",
    "    )\n",
    "\"\"\")\n",
    "dateSeries.createOrReplaceTempView(\"dateSeries\")\n",
    "\n",
    "\n",
    "dfFilled = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        ds.Product_Category, \n",
    "        ds.Date, \n",
    "        COALESCE(sd.Total_Purchases, 0) AS Total_Purchases\n",
    "    FROM dateSeries ds\n",
    "    LEFT JOIN sales_data sd\n",
    "    ON ds.Product_Category = sd.Product_Category AND ds.Date = sd.Date\n",
    "\"\"\")\n",
    "dfFilled.createOrReplaceTempView(\"filled_data\")\n",
    "\n",
    "\n",
    "dfFilled.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\academic\\Spark\\RetailRadar\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Importing plotly failed. Interactive plots will not work.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from prophet import Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainProphetModel(dfFilled):\n",
    "    dfFilled['Date'] = pd.to_datetime(dfFilled['Date'])\n",
    "    models = {}\n",
    "    for category in dfFilled['Product_Category'].unique():\n",
    "        categoryData = dfFilled[dfFilled['Product_Category'] == category]\n",
    "        prophetData = categoryData[['Date', 'Total_Purchases']].rename(columns={'Date': 'ds', 'Total_Purchases': 'y'})\n",
    "        model = Prophet()\n",
    "        model.fit(prophetData)\n",
    "        models[category] = model\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictNext30And7Days(models, dfFilled):\n",
    "    timeSeriesData['Date'] = pd.to_datetime(timeSeriesData['Date'])\n",
    "    predictions = {}\n",
    "    for category, model in models.items():\n",
    "        categoryData = timeSeriesData[timeSeriesData['Product_Category'] == category]\n",
    "        prophetData = categoryData[['Date', 'Total_Purchases']].rename(columns={'Date': 'ds', 'Total_Purchases': 'y'})\n",
    "        lastDate = prophetData['ds'].max()\n",
    "        future = model.make_future_dataframe(periods=30, include_history=False)\n",
    "        future = future[future['ds'] > lastDate]\n",
    "        forecast = model.predict(future)\n",
    "        predictions[category] = forecast[['ds', 'yhat']].assign(Product_Category=category)\n",
    "    \n",
    "    allPredictions = pd.concat(predictions.values())\n",
    "    totalSales30Days = allPredictions.groupby('Product_Category')['yhat'].sum().round().astype(int).reset_index()\n",
    "    totalSales30Days.columns = ['Product_Category', 'Total_Predicted_Sales_30Days']\n",
    "    \n",
    "    next7Days = allPredictions[allPredictions['ds'] <= (allPredictions['ds'].min() + pd.Timedelta(days=6))]\n",
    "    totalSales7Days = next7Days.groupby('Product_Category')['yhat'].sum().round().astype(int).reset_index()\n",
    "    totalSales7Days.columns = ['Product_Category', 'Total_Predicted_Sales_7Days']\n",
    "    \n",
    "    return allPredictions, totalSales30Days, totalSales7Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:46:05 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:46:06 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:46:07 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:46:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:46:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:46:10 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:46:11 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:46:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:46:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:46:14 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total predicted sales for the next 30 days:\n",
      "  Product_Category  Total_Predicted_Sales_30Days\n",
      "0            Books                          9325\n",
      "1         Clothing                          9218\n",
      "2      Electronics                         12000\n",
      "3          Grocery                         11004\n",
      "4       Home Decor                          9121\n",
      "\n",
      "Total predicted sales for the next 7 days:\n",
      "  Product_Category  Total_Predicted_Sales_7Days\n",
      "0            Books                         2155\n",
      "1         Clothing                         2122\n",
      "2      Electronics                         2763\n",
      "3          Grocery                         2542\n",
      "4       Home Decor                         2103\n"
     ]
    }
   ],
   "source": [
    "dfFilled = dfFilled.toPandas()\n",
    "models = trainProphetModel(dfFilled)\n",
    "predictions, totalSales30Days, totalSales7Days = predictNext30And7Days(models, dfFilled)\n",
    "\n",
    "print(\"\\nTotal predicted sales for the next 30 days:\")\n",
    "print(totalSales30Days)\n",
    "print(\"\\nTotal predicted sales for the next 7 days:\")\n",
    "print(totalSales7Days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDates = df.select(\"Date\").distinct()\n",
    "allProducts = df.select(\"Product_Type\").distinct()\n",
    "\n",
    "completeDf = allDates.crossJoin(allProducts)\n",
    "dfComplete = completeDf.join(df, on=[\"Date\", \"Product_Type\"], how=\"left\").na.fill(0)\n",
    "pandas_df = df_complete.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:05:16 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:05:18 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:05:19 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:05:21 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:05:23 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:05:28 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:05:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:05:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:05:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:05:40 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:05:41 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:05:42 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:05:42 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:05:43 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:05:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:05:44 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:05:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:05:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:05:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:05:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:05:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:05:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:05:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:05:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:05:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:05:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:05:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:05:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:05:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:05:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:05:55 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:05:56 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:05:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:05:57 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:05:57 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:05:58 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:05:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:05:59 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:06:00 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:06:00 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:06:01 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:06:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:06:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:06:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:06:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:06:06 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:06:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:06:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:06:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:06:11 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:06:11 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:06:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:06:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:06:15 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:06:18 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:06:20 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:06:24 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:06:26 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:06:29 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:06:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:06:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:06:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:06:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:06:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:06:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:06:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:06:37 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:06:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:06:40 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:06:42 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:06:44 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:06:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:06:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:06:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:06:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:06:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:06:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:06:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:06:57 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:06:58 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:07:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:07:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:07:06 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:07:07 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:07:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:07:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:07:11 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:07:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:07:14 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:07:16 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:07:20 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:07:23 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:07:27 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:07:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:07:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:07:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:07:40 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:07:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:07:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:07:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:07:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:07:57 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:07:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:08:00 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:08:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:08:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:08:05 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:08:07 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:08:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:08:10 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:08:12 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:08:13 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:08:14 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:08:15 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:08:17 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:08:18 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:08:20 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:08:24 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:08:27 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:08:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:08:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:08:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:08:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:08:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:08:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:08:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:08:37 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:08:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:08:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:08:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:08:40 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:08:41 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def forecast_sales(product_df, periods):\n",
    "    product_df = product_df.rename(columns={'Date': 'ds', 'Total_Purchases': 'y'})\n",
    "    model = Prophet()\n",
    "    model.fit(product_df)\n",
    "    future = model.make_future_dataframe(periods=periods)\n",
    "    forecast = model.predict(future)\n",
    "    return forecast[['ds', 'yhat']]\n",
    "\n",
    "\n",
    "product_types = pandas_df['Product_Type'].unique()\n",
    "\n",
    "\n",
    "forecasts_7d = {}\n",
    "forecasts_30d = {}\n",
    "\n",
    "for product in product_types:\n",
    "    product_df = pandas_df[pandas_df['Product_Type'] == product]\n",
    "    forecast_7d = forecast_sales(product_df, periods=7)\n",
    "    forecast_30d = forecast_sales(product_df, periods=30)\n",
    "    forecasts_7d[product] = forecast_7d\n",
    "    forecasts_30d[product] = forecast_30d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 highest-selling products for the next 7 days: ['Shorts', 'Tools', 'T-shirt']\n",
      "Top 3 lowest-selling products for the next 7 days: [\"Children's\", 'BlueStar AC', 'Jacket']\n",
      "Top 3 highest-selling products for the next 30 days: ['Shorts', 'Tools', 'T-shirt']\n",
      "Top 3 lowest-selling products for the next 30 days: [\"Children's\", 'BlueStar AC', 'Thriller']\n"
     ]
    }
   ],
   "source": [
    "def get_top_and_lowest_products(forecasts):\n",
    "    total_sales = {}\n",
    "    for product, forecast in forecasts.items():\n",
    "        total_sales[product] = forecast['yhat'].sum()\n",
    "    \n",
    "    sorted_highest = sorted(total_sales.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_3_highest = [product for product, sales in sorted_highest[:3]]\n",
    "    \n",
    "    sorted_lowest = sorted(total_sales.items(), key=lambda x: x[1])\n",
    "    top_3_lowest = [product for product, sales in sorted_lowest[:3]]\n",
    "    \n",
    "    return top_3_highest, top_3_lowest\n",
    "\n",
    "top_3_highest_7d, top_3_lowest_7d = get_top_and_lowest_products(forecasts_7d)\n",
    "top_3_highest_30d, top_3_lowest_30d = get_top_and_lowest_products(forecasts_30d)\n",
    "\n",
    "print(\"Top 3 highest-selling products for the next 7 days:\", top_3_highest_7d)\n",
    "print(\"Top 3 lowest-selling products for the next 7 days:\", top_3_lowest_7d)\n",
    "print(\"Top 3 highest-selling products for the next 30 days:\", top_3_highest_30d)\n",
    "print(\"Top 3 lowest-selling products for the next 30 days:\", top_3_lowest_30d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
